% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/easy_tool.R
\name{easy_tool}
\alias{easy_tool}
\title{\code{easy_tool} creates the components of an easy-to-use screening tool based
on the results of either \code{lasso_screenr} or \code{logreg_screenr}.}
\usage{
easy_tool(object, max = 3, model = "minAIC", crossval = TRUE, ...)
}
\arguments{
\item{object}{an object of class \code{lasso_screenr} or
\code{logreg_screenr}.}

\item{max}{(numeric) the desired maximum value for the response-question
weights (default is 3).}

\item{model}{(for \verb{lasso_screenr} objects only) the desired basis
model. Valid options are \verb{"minAIC"} (the default) and \verb{"minBIC"}.}

\item{crossval}{a (logical) indicator for cross-validated (\verb{TRUE}) or
in-sample (\verb{FALSE}) performance evaluation.}

\item{...}{additional arguments passed to \code{coef.lasso_screenr} or
\code{coef.logreg_screenr}}
}
\value{
\code{easy_tool} returns (invisibly) an object of class \code{easy_tool}
containing:
\describe{
\item{\code{QuestionWeights}}{Weights for the screening questions obtained
by rescaling the non-zero-valued logistic regression coefficients to whole
numbers ranging from 1 to \code{max}.}
\item{\code{Type}}{The type of test performance evaluaion ("cross-validated"
or "in-sample").}
\item{\code{Scores}}{A data frame containing the testing outcomes
(\code{response}) and cross-validated scores obtained as the sums of the
weighted responses to the set of screening questions (\code{score}).}
\item{\code{ROC}}{An object of class \verb{roc} containing the
receiver-operating characteristic produced by \code{`pROC::roc`}.}
}
}
\description{
\code{easy_tool} creates the components of an easy-to-use screening tool based
on the results of either \code{lasso_screenr} or \code{logreg_screenr}.
}
\details{
The estimates of the coefficients for the screening questions are rescaled to
whole numbers ranging from 1 to \code{max} (\code{QuestionWeights}). Those are
used as weights for each screening question.  The cross-validation results
are then converted to questionnaire scores, where the score for each subject
is the sum of the weighted responses to each question.

The \code{QuestionWeights} are the foundation for easy screening. For example,
the screening tool could consist of a simple questionnaire followed by the
weight for each question, expressed as a small whole number (1, ..., \code{max})
and/or an equal number of open circles.  The person doing the screening need
only circle the numerical weight and/or fill in the circles if and only if the
subject provides a "yes" response to a particular question.  The person doing
the screening then obtains the final score for that subject by adding up the
circled numbers or counting the total number of filled-in circles.  Testing is
mandatory for consenting subjects for whom that final score equals or exceeds
the chosen threshold based on the receiver-operating characteristics of
\code{CVresults}.

The value chosen for \code{max} involves a trade-off between the ease of
manual scoring and the degree to which the ROC from the re-scaling matches the
ROC from the model. Small values of \code{max} make manual scoring easy, and
sufficiently large values will match the screening performance of the model
fit. A value of \verb{3} may be a reasonable compromise. It is prudent to
compare the ROCs from a few values of \code{max} with the ROC from the model
and base the final choice on the trade-off between ease of manual scoring and
the desired combination of sensitivity and specificity.

\code{coef}, \code{ntpp}, \code{plot}, \code{print} and \code{summary}
methods are available for \code{easy_tool}-class objects.
}
\examples{
attach(uniobj1)
tool <- easy_tool(uniobj1, max = 3)
class(tool)
}
\seealso{
\code{`rescale_to_int`}, \code{`coef.easy_tool`}, \code{ntpp.easy_tool},
\code{`plot.easy_tool`}, \code{`print.easy_tool`} and \code{`summary.easy_tool`}
}
