% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lasso_screenr.R
\name{lasso_screenr}
\alias{lasso_screenr}
\title{Fitting Screening Tools Using Lasso-Like Regularization of Logistic Regression}
\usage{
lasso_screenr(
  formula,
  data = NULL,
  Nfolds = 10,
  L2 = TRUE,
  seed = Sys.time(),
  ...
)
}
\arguments{
\item{formula}{an object of class \code{stats::formula} defining the
testing outcome and predictor covariates.}

\item{data}{a dataframe containing the variables defined in \verb{formula}.}

\item{Nfolds}{the number of folds used for \emph{k}-fold cross
validation (default = 10, minimum = 2, maximum = 100).}

\item{L2}{(logical) switch controlling penalization using the \emph{L}2 norm of the
parameters (default = \verb{TRUE}).}

\item{seed}{random number generator seed for cross-validation data splitting.}

\item{...}{additional arguments passed to \code{\link[glmpath]{glmpath}} or
\code{\link[pROC]{roc}}.}
}
\value{
Return (invisibly) an object of class \code{lasso_screenr} containing
the elements:
\describe{
\item{\code{Call}}{The function call.}
\item{\code{Prevalence}}{Prevalence of the binary response variable.}
\item{\code{glmpathObj}}{An object of class \code{glmpath} returned by
\code{glmpath::glmpath}. See \code{help(glmpath)} and
\code{methods(class = "glmpath")}.}
\item{\code{Xmat}}{The matrix of predictors.}
\item{\code{isResults}}{A list structure containing the results from the two
model fits which produced the minimum AIC and BIC values, respectively. The
results consist of \code{Coefficients} (the logit-scale parameter estimates,
including the intercept), \code{isPreds} (the in-sample predicted
probabilities) and \code{isROC} (the in-sample receiver-operating
characteristic (ROC) of class \code{roc}).}
\item{\code{RNG}}{Specification of the random-number generator used for
k-fold data splitting.}
\item{\code{RNGseed}}{RNG seed.}
\item{\code{cvResults}}{A list structure containing the results of \emph{k}-
fold cross-validation estimation of out-of-sample performance.}
}

The list elements of \code{cvResutls} are:
\describe{
    \item{\code{Nfolds}}{the number folds \emph{k}}
    \item{\code{X_ho}}{the matrix of held-out predictors for each cross-validation
fold}
    \item{\code{minAICcvPreds}}{the held-out responses and out-of-sample predicted
probabilities from AIC-best model selection}
    \item{\code{minAICcvROC}}{the out-of-sample ROC object
of class \code{roc} from AIC-best model selection}
    \item{\code{minBICcvPreds}}{the held-out responses and out-of-sample predicted probabilities from
BIC-best model selection}
    \item{\code{minBICcvROC}}{the corresponding out-of-sample predicted probabilities
and ROC object from BIC-best model selection}
}
}
\description{
\code{lasso_screenr} is a convenience function which integrates
logistic regression using \emph{L}1 regularization, \emph{k}-fold
cross-validation and estimation of the receiver-operating characteristic.
The in-sample and out-of-sample performance is estimated from the models
which produced the minimum AIC and minimum BIC.  Execute
\code{methods(class = "lasso_screenr")} to identify available methods.
}
\details{
\code{lasso_screenr} uses the \emph{L}1 path regularizer of
Park and Hastie (2007), as implemented in the \code{glmpath} package.
Park-Hastie regularization is is similar to the conventional lasso and the
elastic net. It differs from the lasso with the inclusion of a very small,
fixed (\verb{1e-5}) penalty on the \emph{L}2 norm of the parameters, and
differs from the elastic net in that the \emph{L}2 penalty is
fixed.  Like the elastic net, the Park-Hastie regularization is robust to
highly correlated predictors. The \emph{L}2 penalization can be turned off
(\code{L2 = FALSE}), in which case the regularization is similar to the
coventional lasso.

The receiver-operating characteristics are computed using the \code{pROC} package.

Out-of-sample performance is estimated using \emph{k}-fold cross-validation.
For a gentle but Python-centric introduction to \emph{k}-fold cross-validation,
see \url{https://machinelearningmastery.com/k-fold-cross-validation/}.
}
\examples{
data(unicorns)
help(unicorns)
uniobj1 <- lasso_screenr(testresult ~ Q1 + Q2 + Q3 + Q4 + Q5 + Q6 + Q7,
                          data = unicorns, Nfolds = 10)
summary(uniobj1)

}
\references{
Park MY, Hastie T. \emph{L}1-regularization path algorithm for generalized linear
models. Journal of the Royal Statistical Society Series B. 2007;69(4):659-677.
\url{https://doi.org/10.1111/j.1467-9868.2007.00607.x}

Robin X, Turck N, Hainard A, Tiberti N, Lisacek F, Sanchez J-C,
MÃ¼ller M. \code{pROC}: An open-source package for \code{R} and S+ to
analyze and compare ROC curves. BMC Bioinformatics. 2011;12(77):1-8.
\url{http://doi.org/10.1186/1471-2105-12-77}
}
\seealso{
\code{\link[glmpath]{glmpath}}, \code{\link[pROC]{roc}}
}
