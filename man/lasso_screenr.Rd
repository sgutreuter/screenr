% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lasso_screenr.R
\name{lasso_screenr}
\alias{lasso_screenr}
\title{Fitting Screening Tools Using Lasso-Like Regularization of Logistic Regression}
\usage{
lasso_screenr(
  formula,
  data = NULL,
  Nfolds = 10,
  L2 = TRUE,
  partial_auc = c(0.8, 1),
  partial_auc_focus = "sensitivity",
  partial_auc_correct = TRUE,
  boot_n = 4000,
  conf_level = 0.95,
  standardize = FALSE,
  seed = Sys.time(),
  ...
)
}
\arguments{
\item{formula}{an object of class \code{stats::formula} defining the
testing outcome and predictor variables.}

\item{data}{a dataframe containing the variables defined in \verb{formula}.
The testing outcome must be binary (0 = no/negative, 1 = yes/positive) or
logical (\verb{FALSE}/\verb{TRUE}).  The the predictor variables are
are typically binary or logical responses to questions which may be
predictive of the test result, but numeric variables can also be used.}

\item{Nfolds}{the number of folds used for \emph{k}-fold cross
validation. Default = 10; minimum = 2, maximum = 100.}

\item{L2}{(logical) switch controlling penalization using the \emph{L}2 norm of
the parameters.  Default: \verb{TRUE}).}

\item{partial_auc}{either a logical \verb{FALSE} or a numeric vector of the
form \code{c(left, right)} where left and right are numbers in the interval
[0, 1] specifying the endpoints for computation of the partial area under the
ROC curve (pAUC). The total AUC is computed if \code{partial_auc} = \verb{FALSE}.
Default: \code{c(0.8, 1.0)}}

\item{partial_auc_focus}{one of \verb{"sensitivity"} or \verb{specificity},
specifying for which the pAUC should be computed.  \code{partial_auc.focus} is
ignored if \code{partial_auc} = \verb{FALSE}.  Default: \verb{"sensitivity"}.}

\item{partial_auc_correct}{logical value indicating whether the pAUC should be
transformed the interval from 0.5 to 1.0. \code{partial_auc_correct} is
ignored if \code{partial_auc} = \verb{FALSE}. Default: \verb{TRUE}).}

\item{boot_n}{number of bootstrap replications for computation of confidence
intervals for the (partial)AUC. Default: 4000.}

\item{conf_level}{a number between 0 and 1 specifying the confidence level
for confidence intervals for the (partial)AUC. Default: 0.95.}

\item{standardize}{logical; if TRUE predictors are standardized to unit
variance.  Default: FALSE (sensible for binary and logical predictors).}

\item{seed}{random number generator seed for cross-validation data splitting.}

\item{...}{additional arguments passed to \code{\link[glmpath]{glmpath}},
\code{\link[pROC]{roc}}, \code{\link[pROC]{auc}} or \code{\link[pROC]{ci}} .}
}
\value{
\code{lasso_screenr} returns (invisibly) an object of class \code{lasso_screenr}
containing the components:
\describe{
\item{\code{Call}}{The function call.}
\item{\code{Prevalence}}{Prevalence of the binary response variable.}
\item{\code{glmpathObj}}{An object of class \code{glmpath} returned by
\code{glmpath::glmpath}. See \code{help(glmpath)} and
\code{methods(class = "glmpath")}.}
\item{\code{Xmat}}{The matrix of predictors.}
\item{\code{isResults}}{A list structure containing the results from the two
model fits which produced the minimum AIC and BIC values, respectively. The
results consist of \code{Coefficients} (the logit-scale parameter estimates,
including the intercept), \code{isPreds} (the in-sample predicted
probabilities) and \code{isROC} (the in-sample receiver-operating
characteristic (ROC) of class \code{roc}).}
\item{\code{RNG}}{Specification of the random-number generator used for
k-fold data splitting.}
\item{\code{RNGseed}}{RNG seed.}
\item{\code{cvResults}}{A list structure containing the results of \emph{k}-
fold cross-validation estimation of out-of-sample performance.}
}

The list elements of \code{cvResutls} are:
\describe{
    \item{\code{Nfolds}}{the number folds \emph{k}}
    \item{\code{X_ho}}{the matrix of held-out predictors for each cross-validation
fold}
    \item{\code{minAICcvPreds}}{the held-out responses and out-of-sample predicted
probabilities from AIC-best model selection}
    \item{\code{minAICcvROC}}{the out-of-sample ROC object
of class \code{roc} from AIC-best model selection}
    \item{\code{minBICcvPreds}}{the held-out responses and out-of-sample predicted probabilities from
BIC-best model selection}
    \item{\code{minBICcvROC}}{the corresponding out-of-sample predicted probabilities
and ROC object from BIC-best model selection}
}
}
\description{
\code{lasso_screenr} is a convenience function which combines
logistic regression using \emph{L}1 regularization, \emph{k}-fold
cross-validation, and estimation of the receiver-operating characteristic (ROC).
The in-sample and out-of-sample performance is estimated from the models
which produced the minimum AIC and minimum BIC.  Execute
\code{methods(class = "lasso_screenr")} to identify available methods.
}
\details{
\code{lasso_screenr} uses the \emph{L}1 path regularizer of
Park and Hastie (2007), as implemented in the \code{glmpath} package.
Park-Hastie regularization is is similar to the conventional lasso and the
elastic net. It differs from the lasso with the inclusion of a very small,
\emph{fixed} (\verb{1e-5}) penalty on the \emph{L}2 norm of the parameter
vector, and differs from the elastic net in that the \emph{L}2 penalty is
fixed.  Like the elastic net, the Park-Hastie regularization is robust to
highly correlated predictors. The \emph{L}2 penalization can be turned off
(\code{L2 = FALSE}), in which case the regularization is similar to the
coventional lasso. Like all \emph{L}1 regularizers, the Park-Hastie
algorithm automatically "deletes" covariates by shrinking their parameter
estimates to 0.

The coefficients produced by \emph{L}1 regularization are biased toward
zero.  Therefore one might consider refitting the model selected by
regularization using maximum-likelihood estimation as implemented in
\code{logreg_screenr}.

The receiver-operating characteristics are computed using the \code{pROC}
package.

Out-of-sample performance is estimated using \emph{k}-fold cross-validation.
For a gentle but Python-centric introduction to \emph{k}-fold cross-validation,
see
\url{https://machinelearningmastery.com/k-fold-cross-validation/}.
}
\examples{
\dontrun{
data(unicorns)
help(unicorns)
uniobj1 <- lasso_screenr(testresult ~ Q1 + Q2 + Q3 + Q4 + Q5 + Q6 + Q7,
                          data = unicorns, Nfolds = 10)
summary(uniobj1)
}

}
\references{
Park MY, Hastie T. \emph{L}1-regularization path algorithm for generalized linear
models. Journal of the Royal Statistical Society Series B. 2007;69(4):659-677.
\url{https://doi.org/10.1111/j.1467-9868.2007.00607.x}

Kim J-H. Estimating classification error rate: Repeated cross-validation, repeated
hold-out and bootstrap. Computational Statistics and Data Analysis.
2009:53(11):3735-3745. \url{http://doi.org/10.1016/j.csda.2009.04.009}

Robin X, Turck N, Hainard A, Tiberti N, Lisacek F, Sanchez J-C,
Muller M. \code{pROC}: An open-source package for \code{R} and S+ to
analyze and compare ROC curves. BMC Bioinformatics. 2011;12(77):1-8.
\url{http://doi.org/10.1186/1471-2105-12-77}
}
\seealso{
\code{\link[glmpath]{glmpath}}, \code{\link[pROC]{roc}},
\code{\link[pROC]{auc}}
}
